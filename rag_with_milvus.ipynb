{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus in ./.venv/lib/python3.12/site-packages (2.4.15)\n",
      "Requirement already satisfied: setuptools>69 in ./.venv/lib/python3.12/site-packages (from pymilvus) (80.9.0)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in ./.venv/lib/python3.12/site-packages (from pymilvus) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in ./.venv/lib/python3.12/site-packages (from pymilvus) (6.33.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in ./.venv/lib/python3.12/site-packages (from pymilvus) (1.2.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in ./.venv/lib/python3.12/site-packages (from pymilvus) (5.11.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in ./.venv/lib/python3.12/site-packages (from pymilvus) (2.3.3)\n",
      "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from pymilvus) (2.4.12)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from milvus-lite<2.5.0,>=2.4.0->pymilvus) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, db, Collection , utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_db not in current dbs list, creating one!!!\n"
     ]
    }
   ],
   "source": [
    "connections.add_connection(\n",
    "    rag_conn = {\n",
    "        \"host\":\"localhost\",\n",
    "        \"port\":19530,\n",
    "        \"username\":\"\",\n",
    "        \"password\":\"\"\n",
    "        }\n",
    ")\n",
    "\n",
    "conn_name = 'rag_conn'\n",
    "db_name = 'rag_db'\n",
    "\n",
    "connections.connect(conn_name)\n",
    "connections.list_connections()\n",
    "\n",
    "current_dbs = db.list_database(using=conn_name)\n",
    "\n",
    "if db_name not in current_dbs:\n",
    "    print(f\"{db_name} not in current dbs list, creating one!!!\")\n",
    "    resume_db = db.create_database(db_name, using=conn_name)\n",
    "\n",
    "db.using_database(db_name, using= conn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections:  ['rag_collection']\n",
      "Schema:  {'auto_id': False, 'description': 'rag_schema', 'fields': [{'name': 'chunk_id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'rag_text_field', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 2048}}, {'name': 'rag_embedding_field', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, Collection, utility, DataType\n",
    "\n",
    "chunk_id = FieldSchema(\n",
    "    name='chunk_id',\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary = True,\n",
    "    max_length = 32\n",
    ")\n",
    "\n",
    "rag_text_field = FieldSchema(\n",
    "    name='rag_text_field',\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length = 2048\n",
    ")\n",
    "\n",
    "rag_embedding_field = FieldSchema(\n",
    "    name ='rag_embedding_field',\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim = 384\n",
    ")\n",
    "\n",
    "rag_schema = CollectionSchema(\n",
    "    fields=[chunk_id, rag_text_field, rag_embedding_field],\n",
    "    description='rag_schema',\n",
    "    enable_dynamic_field = True\n",
    ")\n",
    "\n",
    "collection_name = 'rag_collection'\n",
    "\n",
    "rag_collection = Collection(\n",
    "    name='rag_collection',\n",
    "    schema=rag_schema,\n",
    "    using= conn_name,\n",
    "    shard_nums = 2\n",
    ")\n",
    "\n",
    "print(\"Collections: \", utility.list_collections(using=conn_name))\n",
    "\n",
    "r_collection = Collection(collection_name, using=conn_name )\n",
    "print(\"Schema: \", r_collection.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep For Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\"LLM_Demo.pdf\")\n",
    "pdf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': 'LLM_Demo.pdf', 'file_path': 'LLM_Demo.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'LLM_Demo', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Large Language Models, commonly known as LLMs, represent one of the most significant \\nmilestones in the evolution of artificial intelligence, allowing machines to understand, reason \\nwith, and generate human-like language across an astonishing variety of tasks. These models, \\nsuch as GPT, Gemini, Claude, and Mistral, share a common foundation built upon the \\nTransformer architecture introduced in 2017 in the groundbreaking paper “Attention Is All You \\nNeed.” The Transformer replaced recurrent and convolutional architectures with a mechanism \\nknown as self-attention, enabling each word or token in a sequence to interact with every other \\ntoken based on learned relevance scores. During training, the model consumes vast amounts of \\ntextual data drawn from the internet, books, and other corpora, transforming it into \\nembeddings—dense numerical vectors that encode meaning, syntax, and context. The learning \\nprocess involves predicting the next token in a sequence given its context, an optimization task \\nsolved using gradient descent across billions or even trillions of parameters distributed across \\nmassive clusters of GPUs or TPUs. Over time, the LLM internalizes statistical relationships and \\nconceptual structures that allow it to generate coherent, contextually relevant responses, even \\nin situations it has never explicitly seen before. \\nOnce trained, these models exhibit remarkable versatility. They can summarize lengthy \\ndocuments, compose essays, translate languages, write computer programs, analyze legal \\ncontracts, and even generate poetry. This breadth of ability emerges not from explicit \\nrule-writing but from the probabilistic understanding of linguistic patterns embedded in their \\nparameter space. The architecture itself is modular, typically consisting of an encoder and \\ndecoder stack, with layers of multi-head attention, feed-forward networks, normalization, and \\npositional encoding mechanisms. The self-attention mechanism remains the beating heart of \\nthis design, computing relationships between every pair of tokens to create context-aware \\nrepresentations that evolve across layers. Each token’s embedding becomes a multidimensional \\nsummary of meaning shaped by all surrounding words, allowing the model to exhibit long-range \\nreasoning without sequential bottlenecks typical of RNNs. \\nDespite their power, LLMs face critical limitations, the most notable being their restricted context \\nwindow and static knowledge base. Since the model can only process a limited number of \\ntokens in a single inference pass and cannot access information beyond its training cutoff, \\nresearchers developed a hybrid paradigm called Retrieval-Augmented Generation (RAG). In \\nthis approach, instead of relying entirely on memorized data, the model interacts with an \\nexternal knowledge retrieval system. When a user issues a query, the system first generates an \\nembedding of the query and searches a vector database—often implemented using FAISS, \\nMilvus, or Pinecone—to locate semantically similar text chunks from a curated corpus. These \\nretrieved segments are then appended to the model’s prompt so that its generation phase can \\nbe grounded in up-to-date, factual content. This mechanism essentially grants the model \\ndynamic memory, bridging the gap between static language understanding and contextually \\naware reasoning. The pipeline can be viewed as consisting of two cooperating agents: the \\nretriever, responsible for semantic search, and the generator, which synthesizes the final \\ncoherent answer using both the retrieved context and its pretrained linguistic understanding. \\nSuch a setup significantly reduces hallucinations, improves factual consistency, and enables'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': 'LLM_Demo.pdf', 'file_path': 'LLM_Demo.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'LLM_Demo', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='applications where accuracy and domain specificity are critical, such as law, finance, healthcare, \\nand research analytics. \\nIn industrial settings, the Transformer family has expanded into numerous variants: \\nencoder-only models like BERT optimized for classification, decoder-only architectures like GPT \\noptimized for generation, and encoder-decoder hybrids like T5 for translation and \\nsummarization. The performance scaling law observed across these models shows a \\nnear-power-law relationship between dataset size, model parameters, and compute resources, \\nleading to steady improvements in linguistic competence as capacity increases. However, larger \\nmodels also bring challenges of efficiency, interpretability, and environmental impact. To mitigate \\nthese, new training techniques such as quantization, low-rank adaptation (LoRA), parameter \\nsharing, and mixture-of-experts have been introduced. Fine-tuning strategies allow small, \\ndomain-specific models to achieve near state-of-the-art results using only a fraction of \\nresources. Furthermore, the integration of RAG architectures with frameworks like LangChain \\nand LlamaIndex has democratized development of intelligent, domain-grounded applications, \\nenabling even small teams to deploy powerful AI agents capable of document comprehension, \\ndecision support, and real-time reasoning over enterprise data. \\nThe applications of LLMs span almost every field imaginable. In conversational AI, chatbots \\npowered by models like GPT or Claude now handle millions of daily queries, demonstrating \\nempathy, reasoning, and contextual awareness that approach human quality. In content \\ngeneration, marketers and journalists rely on these systems to draft narratives, blogs, and \\nreports at unprecedented scale. In software engineering, tools such as Copilot and Gemini \\nCode accelerate development cycles by generating functional code, suggesting optimizations, \\nand even writing unit tests. In search and knowledge retrieval, LLMs augment traditional \\nkeyword-based engines by understanding semantic intent, producing synthesized, context-rich \\nanswers rather than lists of documents. In healthcare, models trained on biomedical corpora \\nassist doctors in literature reviews and case analyses. The combination of generation and \\nretrieval means that, for the first time, machines can reason over both stored memory and \\ndynamic context simultaneously. \\nLooking ahead, the evolution of large language models is accelerating toward multimodal \\nintelligence. Future systems will not be limited to text—they will integrate images, audio, video, \\nand sensor data into unified representations, enabling comprehension across modalities. The \\nnext generation of models will also be more efficient, trained through techniques like \\nreinforcement learning from human feedback (RLHF), parameter-efficient fine-tuning, and \\nself-distillation. Multi-agent systems built upon these models will coordinate autonomously to \\nperform complex, multi-step tasks such as research synthesis, workflow automation, and \\nsimulation of social dynamics. Agentic AI, where autonomous agents communicate and \\ncollaborate using natural language, will become a defining paradigm. As these systems evolve, \\nthe role of RAG will remain central, serving as the interface between the structured world of \\ndatabases and the unstructured world of human communication. By merging symbolic \\nreasoning with neural generation, RAG-enhanced architectures effectively turn LLMs into living \\nsystems of dynamic knowledge, continually expanding and adapting to new information without \\nfull retraining.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': 'LLM_Demo.pdf', 'file_path': 'LLM_Demo.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'LLM_Demo', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='The future of artificial intelligence therefore lies not in a single model but in ecosystems of \\ninteroperable intelligence: retrieval modules, reasoning engines, and generative cores \\nconnected through APIs, memory layers, and feedback loops. Tools like Hugging Face \\nTransformers, OpenAI API, and Google Vertex AI have already begun this modularization, \\nempowering developers to assemble AI workflows as easily as they compose web applications. \\nAs these technologies mature, the boundary between data engineering, machine learning, and \\nsoftware development will blur, creating a new discipline of end-to-end AI systems engineering. \\nEfficiency will continue to improve through hardware innovation, while responsible AI initiatives \\nwill focus on reducing bias, ensuring transparency, and aligning model behavior with human \\nvalues. \\nUltimately, large language models are no longer static text predictors—they are the cognitive \\nsubstrate of the digital age. When fused with retrieval mechanisms, they evolve from \\nmemory-bound parrots into adaptive reasoning entities capable of drawing upon the collective \\nknowledge of humanity. Their influence extends from research and education to art, policy, and \\nethics. As LLMs and RAG architectures converge, the distinction between stored information \\nand generated knowledge will fade, giving rise to intelligent agents that not only understand the \\nworld but continuously learn from it, transforming the way humans and machines collaborate to \\ncreate, discover, and think.')]\n"
     ]
    }
   ],
   "source": [
    "print(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (1.0.5)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.4 in ./.venv/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in ./.venv/lib/python3.12/site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (0.4.41)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in ./.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in ./.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in ./.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in ./.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in ./.venv/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in ./.venv/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.venv/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "cleaned_data = []\n",
    "def clean_pdf_text(pdf_docs):\n",
    "    for d in pdf_docs:\n",
    "        text = d.page_content\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.replace(\" \\n\", \" \").replace(\"\\n \", \" \").replace(\"\\n\", \" \")\n",
    "        text = text.strip()\n",
    "        cleaned_data.append(Document(page_content = text, metadata = d.metadata))\n",
    "    return cleaned_data\n",
    "\n",
    "pdf_docs = clean_pdf_text(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': 'LLM_Demo.pdf', 'file_path': 'LLM_Demo.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'LLM_Demo', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Large Language Models, commonly known as LLMs, represent one of the most significant milestones in the evolution of artificial intelligence, allowing machines to understand, reason with, and generate human-like language across an astonishing variety of tasks. These models, such as GPT, Gemini, Claude, and Mistral, share a common foundation built upon the Transformer architecture introduced in 2017 in the groundbreaking paper “Attention Is All You Need.” The Transformer replaced recurrent and convolutional architectures with a mechanism known as self-attention, enabling each word or token in a sequence to interact with every other token based on learned relevance scores. During training, the model consumes vast amounts of textual data drawn from the internet, books, and other corpora, transforming it into embeddings—dense numerical vectors that encode meaning, syntax, and context. The learning process involves predicting the next token in a sequence given its context, an optimization task solved using gradient descent across billions or even trillions of parameters distributed across massive clusters of GPUs or TPUs. Over time, the LLM internalizes statistical relationships and conceptual structures that allow it to generate coherent, contextually relevant responses, even in situations it has never explicitly seen before. Once trained, these models exhibit remarkable versatility. They can summarize lengthy documents, compose essays, translate languages, write computer programs, analyze legal contracts, and even generate poetry. This breadth of ability emerges not from explicit rule-writing but from the probabilistic understanding of linguistic patterns embedded in their parameter space. The architecture itself is modular, typically consisting of an encoder and decoder stack, with layers of multi-head attention, feed-forward networks, normalization, and positional encoding mechanisms. The self-attention mechanism remains the beating heart of this design, computing relationships between every pair of tokens to create context-aware representations that evolve across layers. Each token’s embedding becomes a multidimensional summary of meaning shaped by all surrounding words, allowing the model to exhibit long-range reasoning without sequential bottlenecks typical of RNNs. Despite their power, LLMs face critical limitations, the most notable being their restricted context window and static knowledge base. Since the model can only process a limited number of tokens in a single inference pass and cannot access information beyond its training cutoff, researchers developed a hybrid paradigm called Retrieval-Augmented Generation (RAG). In this approach, instead of relying entirely on memorized data, the model interacts with an external knowledge retrieval system. When a user issues a query, the system first generates an embedding of the query and searches a vector database—often implemented using FAISS, Milvus, or Pinecone—to locate semantically similar text chunks from a curated corpus. These retrieved segments are then appended to the model’s prompt so that its generation phase can be grounded in up-to-date, factual content. This mechanism essentially grants the model dynamic memory, bridging the gap between static language understanding and contextually aware reasoning. The pipeline can be viewed as consisting of two cooperating agents: the retriever, responsible for semantic search, and the generator, which synthesizes the final coherent answer using both the retrieved context and its pretrained linguistic understanding. Such a setup significantly reduces hallucinations, improves factual consistency, and enables'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': 'LLM_Demo.pdf', 'file_path': 'LLM_Demo.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'LLM_Demo', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='applications where accuracy and domain specificity are critical, such as law, finance, healthcare, and research analytics. In industrial settings, the Transformer family has expanded into numerous variants: encoder-only models like BERT optimized for classification, decoder-only architectures like GPT optimized for generation, and encoder-decoder hybrids like T5 for translation and summarization. The performance scaling law observed across these models shows a near-power-law relationship between dataset size, model parameters, and compute resources, leading to steady improvements in linguistic competence as capacity increases. However, larger models also bring challenges of efficiency, interpretability, and environmental impact. To mitigate these, new training techniques such as quantization, low-rank adaptation (LoRA), parameter sharing, and mixture-of-experts have been introduced. Fine-tuning strategies allow small, domain-specific models to achieve near state-of-the-art results using only a fraction of resources. Furthermore, the integration of RAG architectures with frameworks like LangChain and LlamaIndex has democratized development of intelligent, domain-grounded applications, enabling even small teams to deploy powerful AI agents capable of document comprehension, decision support, and real-time reasoning over enterprise data. The applications of LLMs span almost every field imaginable. In conversational AI, chatbots powered by models like GPT or Claude now handle millions of daily queries, demonstrating empathy, reasoning, and contextual awareness that approach human quality. In content generation, marketers and journalists rely on these systems to draft narratives, blogs, and reports at unprecedented scale. In software engineering, tools such as Copilot and Gemini Code accelerate development cycles by generating functional code, suggesting optimizations, and even writing unit tests. In search and knowledge retrieval, LLMs augment traditional keyword-based engines by understanding semantic intent, producing synthesized, context-rich answers rather than lists of documents. In healthcare, models trained on biomedical corpora assist doctors in literature reviews and case analyses. The combination of generation and retrieval means that, for the first time, machines can reason over both stored memory and dynamic context simultaneously. Looking ahead, the evolution of large language models is accelerating toward multimodal intelligence. Future systems will not be limited to text—they will integrate images, audio, video, and sensor data into unified representations, enabling comprehension across modalities. The next generation of models will also be more efficient, trained through techniques like reinforcement learning from human feedback (RLHF), parameter-efficient fine-tuning, and self-distillation. Multi-agent systems built upon these models will coordinate autonomously to perform complex, multi-step tasks such as research synthesis, workflow automation, and simulation of social dynamics. Agentic AI, where autonomous agents communicate and collaborate using natural language, will become a defining paradigm. As these systems evolve, the role of RAG will remain central, serving as the interface between the structured world of databases and the unstructured world of human communication. By merging symbolic reasoning with neural generation, RAG-enhanced architectures effectively turn LLMs into living systems of dynamic knowledge, continually expanding and adapting to new information without full retraining.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': 'LLM_Demo.pdf', 'file_path': 'LLM_Demo.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'LLM_Demo', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='The future of artificial intelligence therefore lies not in a single model but in ecosystems of interoperable intelligence: retrieval modules, reasoning engines, and generative cores connected through APIs, memory layers, and feedback loops. Tools like Hugging Face Transformers, OpenAI API, and Google Vertex AI have already begun this modularization, empowering developers to assemble AI workflows as easily as they compose web applications. As these technologies mature, the boundary between data engineering, machine learning, and software development will blur, creating a new discipline of end-to-end AI systems engineering. Efficiency will continue to improve through hardware innovation, while responsible AI initiatives will focus on reducing bias, ensuring transparency, and aligning model behavior with human values. Ultimately, large language models are no longer static text predictors—they are the cognitive substrate of the digital age. When fused with retrieval mechanisms, they evolve from memory-bound parrots into adaptive reasoning entities capable of drawing upon the collective knowledge of humanity. Their influence extends from research and education to art, policy, and ethics. As LLMs and RAG architectures converge, the distinction between stored information and generated knowledge will fade, giving rise to intelligent agents that not only understand the world but continuously learn from it, transforming the way humans and machines collaborate to create, discover, and think.')]\n"
     ]
    }
   ],
   "source": [
    "print(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 22\n",
      "Sample Chunk Text: .” The Transformer replaced recurrent and convolutional architectures with a mechanism known as self-attention, enabling each word or token in a sequence to interact with every other token based on learned relevance scores. During training, the model consumes vast amounts of textual data drawn from the internet, books, and other corpora, transforming it into embeddings—dense numerical vectors that encode meaning, syntax, and context\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 32,\n",
    "    length_function = len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"]\n",
    ")\n",
    "\n",
    "pdf_docs = text_splitter.split_documents(pdf_docs)\n",
    "\n",
    "rag_text = []\n",
    "for i in pdf_docs:\n",
    "    rag_text.append(i.page_content)\n",
    "\n",
    "print(\"Total Chunks:\", len(rag_text))\n",
    "print(\"Sample Chunk Text:\", rag_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarshsingh/LLM_Foundation_RAG/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "rag_embeddings = [embedding_model.encode(i) for i in rag_text]\n",
    "record_ids = [i for i in range(len(rag_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_rows': 22,\n",
       " 'indexed_rows': 22,\n",
       " 'pending_index_rows': 0,\n",
       " 'state': 'Finished'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_data = [record_ids, rag_text, rag_embeddings]\n",
    "\n",
    "i_collection = Collection(collection_name, using=conn_name)\n",
    "\n",
    "mr = i_collection.insert(insert_data)\n",
    "i_collection.flush()\n",
    "\n",
    "\n",
    "index_params = {\n",
    "    'index_type':'IVF_FLAT',\n",
    "    'metric_type':'L2',\n",
    "    'params':{\"nlist\":1024}\n",
    "}\n",
    "\n",
    "i_collection.create_index(\n",
    "    field_name=\"rag_embedding_field\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "utility.index_building_progress(collection_name, using=conn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering Questions with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Results: id: 2, distance: 1.1279608011245728, entity: {'rag_text_field': '. The learning process involves predicting the next token in a sequence given its context, an optimization task solved using gradient descent across billions or even trillions of parameters distributed across massive clusters of GPUs or TPUs. Over time, the LLM internalizes statistical relationships and conceptual structures that allow it to generate coherent, contextually relevant responses, even in situations it has never explicitly seen before. Once trained, these models exhibit remarkable versatility'}\n"
     ]
    }
   ],
   "source": [
    "search_params = {\n",
    "    'metric_type':'L2',\n",
    "    'offset':0,\n",
    "    'ignore_growing':False,\n",
    "    'params':{\"nprobe\":20}\n",
    "}\n",
    "\n",
    "\n",
    "query = \"what is llm used for?\"\n",
    "search_embedding = embedding_model.encode(query)\n",
    "\n",
    "q_collection = Collection(collection_name, using=conn_name)\n",
    "q_collection.load()\n",
    "\n",
    "results = q_collection.search(\n",
    "    data = [search_embedding],\n",
    "    param= search_params,\n",
    "    anns_field=\"rag_embedding_field\",\n",
    "    limit=4,\n",
    "    # offset =0,\n",
    "    expr= None,\n",
    "    output_fields=['rag_text_field'],\n",
    "    consistency_level = 'Strong'\n",
    ")\n",
    "# print(results)\n",
    "\n",
    "print(\"Top Results:\", results[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Prompt for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the Query by user here : what is llm used for? you have to answer the question below with this context:['. The learning process involves predicting the next token in a sequence given its context, an optimization task solved using gradient descent across billions or even trillions of parameters distributed across massive clusters of GPUs or TPUs. Over time, the LLM internalizes statistical relationships and conceptual structures that allow it to generate coherent, contextually relevant responses, even in situations it has never explicitly seen before. Once trained, these models exhibit remarkable versatility', 'Large Language Models, commonly known as LLMs, represent one of the most significant milestones in the evolution of artificial intelligence, allowing machines to understand, reason with, and generate human-like language across an astonishing variety of tasks. These models, such as GPT, Gemini, Claude, and Mistral, share a common foundation built upon the Transformer architecture introduced in 2017 in the groundbreaking paper “Attention Is All You Need', '. Each token’s embedding becomes a multidimensional summary of meaning shaped by all surrounding words, allowing the model to exhibit long-range reasoning without sequential bottlenecks typical of RNNs. Despite their power, LLMs face critical limitations, the most notable being their restricted context window and static knowledge base', '. The applications of LLMs span almost every field imaginable. In conversational AI, chatbots powered by models like GPT or Claude now handle millions of daily queries, demonstrating empathy, reasoning, and contextual awareness that approach human quality. In content generation, marketers and journalists rely on these systems to draft narratives, blogs, and reports at unprecedented scale']\n"
     ]
    }
   ],
   "source": [
    "context = []\n",
    "\n",
    "for i in range(len(results[0])):\n",
    "    context.append(results[0][i].entity.get('rag_text_field'))\n",
    "\n",
    "prompt = (f\"Based on the Query by user here : '{query}' you have to answer the question below with this context: '{context}' \")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided context, Large Language Models (LLMs) are used for a variety of tasks, including:\\n\\n1. Conversational AI: Chatbots powered by LLMs like GPT or Claude handle millions of daily queries, demonstrating empathy, reasoning, and contextual awareness that approach human quality.\\n2. Content generation: Marketers and journalists rely on LLMs to draft narratives, blogs, and reports at unprecedented scale.\\n3. Language understanding and reasoning: LLMs exhibit long-range reasoning without sequential bottlenecks typical of RNNs, allowing them to understand and reason with complex language inputs.\\n4. Human-like language generation: LLMs can generate coherent, contextually relevant responses, even in situations they have never explicitly seen before.\\n\\nOverall, LLMs are versatile models that can be applied to a wide range of tasks, from conversational AI to content generation, and are capable of understanding and generating human-like language.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-11-11T16:49:51.677438Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 12910958333, 'load_duration': 178710208, 'prompt_eval_count': 355, 'prompt_eval_duration': 6493586542, 'eval_count': 190, 'eval_duration': 5936486083} id='lc_run--811ea273-e1de-40fc-935e-0b3890c671c2-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(temperature = 0.0,model='llama3.2')\n",
    "\n",
    "result_complete = llm.invoke(prompt)\n",
    "\n",
    "print(result_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
